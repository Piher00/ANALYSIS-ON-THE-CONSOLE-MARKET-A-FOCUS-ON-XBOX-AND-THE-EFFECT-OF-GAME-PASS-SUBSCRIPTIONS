#------------------ Neural Network AutoRegressive (see forecast book)--------
nn <- nnetar(gp.ts, lambda=0)
plot(forecast(nn,PI=T))
#bivariate
# Loop over the variables
for (var in variables) {
# Boxplot for each genre_label category
p <- ggplot(data_train, aes_string(x = "genre_label", y = var)) +
geom_boxplot(fill="green", color="black") +
labs(title=paste("Boxplot of", var, "by genre_label"), x="Genre Label", y=var)
# Plot
print(p)
}
rm(list=ls())
library(tidytext)
library(tidyverse)
library(tm)
library(textstem)
data<-read.csv(file.choose())
#
# data<-data[!data$track_name=='Bookend',]
# data<-data[!data$track_name=='Motel Way of Life',]
# data<-data[!data$track_name=='One Of Those Things',]
# data<-data[!data$track_name=='SCRAP BOOOO',]
# data<-data[!data$track_name=='Never Knew',]
# data<-data[!data$track_name=='Low (feat. T-Pain)',]
# data<-data[!data$track_name=='Minutes to Midnight (2015 Remaster)',]
# data<-data[!data$track_name=='Crazy In Love (feat. Jay-Z)',]
# data<-data[!data$track_name=='Cold Heart - PNAU Remix',]
# data<-data[!data$track_name=='Superhero (Heroes & Villains) [with Future & Chris Brown]',]
# data<-data[!data$track_name=='Would? (2022 Remaster)',]
# data<-data[!data$track_name=='This Charming Man (2008 Remastered Version)',]
# data<-data[!data$track_name=='FUNERAL,Blackout Problems',]
# data<-data[!data$track_name=="Protect Ya Neck (feat. RZA, Method Man, Inspectah Deck, Raekwon, U-God, Ol' Dirty Bastard, Ghostface Killah & GZA)",]
# data<-data[!data$track_name=='Shook Ones, Pt. II',]
# data<-data[!data$track_name=='MMMBop',]
# data<-data[!data$track_name=='Desire - Gryffin Remix',]
# data<-data[!data$track_uri=='spotify:track:0LWkaEyQRkF0XAms8Bg1fC',]
# data<-data[!data$track_uri=='spotify:track:2mKouqwAIdQnMP43zxR89r',]
# data<-data[!data$track_uri=='spotify:track:6ctWstoouxCcvuTsd4cHNS',]
# data<-data[!data$track_uri=='spotify:track:7xPGvZaG9W7UOrCgEwbONe',]
# data<-data[!data$track_uri=='spotify:track:78jPDfvrMDeq7TIYOIl8Vf',]
# data<-data[!data$track_uri=='spotify:track:0Qm0HueRJW1O6jjgIUY95N',]
# data<-data[!data$track_uri=='spotify:track:2uZvZlMK9LPw4gpqdjRN7L',]
# data<-data[!data$track_uri=='spotify:track:5UE025ItMWGNWrWwTWTVlc',]
# data<-data[!data$track_uri=='spotify:track:3aQGknD9CVAtpQFh6Qtalq',]
# data<-data[!data$track_uri=='spotify:track:4rhdtzBmrrDxYi35yQDpzF',]
# data<-data[!data$track_uri=='spotify:track:0CAfXk7DXMnon4gLudAp7J',]
# data<-data[!data$track_uri=='spotify:track:2Cd9iWfcOpGDHLz6tVA3G4',]
# data<-data[!data$track_uri=='spotify:track:4Li2WHPkuyCdtmokzW2007',]
# data<-data[!data$track_uri=='spotify:track:5IVuqXILoxVWvWEPm82Jxr',]
# data<-data[!data$track_uri=='spotify:track:2y5aJvzXhHPA94U5GFAcXe',]
# data<-data[!data$track_uri=='spotify:track:0upFohXrGxIIAjyaJmCkMU',]
# data<-data[!data$track_uri=='spotify:track:3UnYMo1aeD0o8VtHy3R8cs',]
# data<-data[!data$track_uri=='spotify:track:74fV8TuLZKVzSIOOGu8wwI',]
# data<-data[!data$track_uri=='spotify:track:0TwBtDAWpkpM3srywFVOV5',]
# data<-data[!data$track_uri=='spotify:track:0lnxrQAd9ZxbhBBe7d8FO8',]
#
# data<-data[!data$track_uri=='spotify:track:4cG7HUWYHBV6R6tHn1gxrl',]
# data<-data[!data$track_uri=='spotify:track:6rPO02ozF3bM7NnOV4h6s2',]
# data<-data[!data$track_uri=='spotify:track:3wif59cWy7x8lPxalb1hac',]
# data<-data[!data$track_uri=='spotify:track:5eNGq77TvvAYYUjg2GX6iM',]
# data<-data[!data$track_uri=='spotify:track:6SEizXg6WimNyK8NdI9biV',]
# data<-data[!data$track_uri=='spotify:track:0CAfXk7DXMnon4gLudAp7J',]
# data<-data[!data$track_uri=='spotify:track:6bNB5gxFX6Q87DbQWb8OWZ',]
# data<-data[!data$track_uri=='spotify:track:14yjulPTB0DlfgphCT6d7g',]
# data<-data[!data$track_uri=='spotify:track:6leLgbDLGnK0K6B9FxovNM',]
# data<-data[!data$track_uri=='spotify:track:2F2tSlt14fYqG8XC2qFyse',]
# data<-data[!data$track_uri=='spotify:track:0T7vRa9aRXSKEtOwfWdF9R',]
# data<-data[!data$track_uri=='spotify:track:6JV2JOEocMgcZxYSZelKcc',]
# data<-data[!data$track_uri=='spotify:track:5IVuqXILoxVWvWEPm82Jxr',]
# data<-data[!data$track_uri=='spotify:track:6z6ZmjqANd9t0bbbLrn4y2',]
# data<-data[!data$track_uri=='spotify:track:688QpQAwqsLdBR6lr13wK1',]
# data<-data[!data$track_uri=='spotify:track:0DYRFd0gGD0v0ITgVdvCso',]
# data<-data[!data$track_uri=='spotify:track:4z7P3XXNrPz4zyCb7BYQkX',]
# data<-data[!data$track_uri=='spotify:track:2IJwjtJnUXAGhUMaZUw4is',]
# data<-data[!data$track_uri=='spotify:track:4cG7HUWYHBV6R6tHn1gxr',]
# data<-data[!data$track_uri=='spotify:track:6JV2JOEocMgcZxYSZelKcc',]
# data<-data[!data$track_uri=='spotify:track:1ZlHr9FYHT7YTbXCoxPq5C',]
# data<-data[!data$track_uri=='spotify:track:2CtemffYhT0DJWcT1XW047',]
# data<-data[!data$track_uri=='spotify:track:1Sgj10byiGzPpI2IrXSFEn',]
# data<-data[!data$track_uri=='spotify:track:2a1o6ZejUi8U3wzzOtCOYw',]
# data<-data[!data$track_uri=='spotify:track:3EkWM4WEQhHPqXJ3GDkBLy',]
#
# data<-data[!data$track_uri=='spotify:track:2Ny0tCcYlti1RXsrNtRMl3',]
# data<-data[!data$track_uri=='spotify:track:74pjVW9vrHneGza82IB5Hx',]
# data<-data[!data$track_uri=='spotify:track:7EhkL8HCRRKWXyfBLwjazs',]
# data<-data[!data$track_uri=='spotify:track:4bG8gUgiCbnGrcjgdBrm3F',]
# data<-data[!data$track_uri=='spotify:track:0lvNoa29MPRpkZvcSG2cB5',]
# data<-data[!data$track_uri=='spotify:track:0WVpOA5QIf4HbRDyZVRsMG',]
# data<-data[!data$track_uri=='spotify:track:5bdIYjZy0ea7UtdDA2t4U7',]
# data<-data[!data$track_uri=='spotify:track:5tz69p7tJuGPeMGwNTxYuV',]
# data<-data[!data$track_uri=='spotify:track:6HfOzLLjsaXsehIFEsrxTk',]
# data<-data[!data$track_uri=='spotify:track:3uPMTWYLuZ4YV2D8NIHQho',]
# data<-data[!data$track_uri=='spotify:track:4WuOWVnAqvEQxgSRrspBgt',]
# data<-data[!data$track_uri=='spotify:track:4agyvayocGtsHaf94oCLq',]
# data<-data[!data$track_uri=='spotify:track:5VR7R8L5MivClO1VxZdZFx',]
# data<-data[!data$track_uri=='spotify:track:4HZ7a4lVS1xXthyqfLopS4',]
# data<-data[!data$track_uri=='spotify:track:4ea9w8c4ROqiZpJVhfBA3m',]
# data<-data[!data$track_uri=='spotify:track:47SYIBabdtGAnvaTXn7N0h',]
#data  <- data[!data$track_uri=='spotify:track:32IiFFXwFaOSCyCLWkMy6x',]
#data <- data[!data$track_uri=='spotify:track:1cOeP9lmNXhyHxudq2WSZi',]
data$X<-NULL
data$Unnamed..0<-NULL
data$Unnamed..0_x<-NULL
data$Unnamed..0_y<-NULL
data<-data[!data$genre_label=='',]
data<-data[!duplicated(data$track_uri),]
data <- data[data$lyrics_IO==1,]
data$lyrics_IO <- NULL
names(data)[names(data)=="key"] <- "key_song"
data$genre_label<-as.factor(data$genre_label)
data$mode<-as.factor(data$mode)
set.seed(1234)
train_dim<-round(NROW(data)*0.8)
random<-sample(1:NROW(data), train_dim)
data_train<-data[random,]
data_test<-data[-random,]
y<-as.factor(data$genre_label)
y.train <- y[random]
y.test <- y[-random]
create_corpus <- function(df){
word_dataset_1<-tibble(text=df$lyric)
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\w*Lyrics\\w*", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\w*Contributors\\w*\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\w*contributors\\w*\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\Contributors\\w*\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\contributors\\w*\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="Lyrics", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="lyrics", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\w*Türkçe\\w*\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\w*bokmål\\w*\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\w*Embed\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="[[:punct:]]", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="Chorus", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="chorus", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="Verse", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="verse", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\[.*?\\]", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\(.*?\\)", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\w*\n\\w*", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="[\r\n]", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\w*Contributor\\w*\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\w*contributor\\w*\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\Contributor\\w*\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\contributor\\w*\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="[[:digit:]]+", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\s+", replacement=" ", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="[[:digit:]]", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\bah\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\boh\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\buh\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\bhuh\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\bna\\b", replacement="", ignore.case=T))
#word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\byeah\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\bem\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\bmm\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\bwhoa\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="also", replacement="", ignore.case=T))
word_dataset_1 <- word_dataset_1 %>% mutate(text = str_replace_all(text, pattern = "[^A-Za-z\\s]", replacement = ""))
# Remove Japanese, Korean, and special characters
x<-VectorSource(word_dataset_1$text)
corpus<-tm::VCorpus(x, readerControl = list(reader = reader(x), language = c('en')))
# Remove Japanese, Korean, and special characters
removeSpecialCharacters <- function(x) {
x <- gsub("[\u3040-\u309F]|[\u30A0-\u30FF]|[\uFF00-\uFF9F]|[\u4E00-\u9FAF]", "", x) # Remove Japanese characters
x <- gsub("[\uAC00-\uD7A3]", "", x) # Remove Korean characters
gsub("[^[:alnum:][:space:]]*", "", x) # Remove special characters
}
corpus <- tm_map(corpus, content_transformer(removeSpecialCharacters))
corpus <- tm_map(corpus, content_transformer(removeSpecialCharacters))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords::stopwords("en", source = "stopwords-iso"))
corpus <- tm_map(corpus, stemDocument, language = c('english'))
corpus <- tm_map(corpus, content_transformer(lemmatize_words), language='en')
return(corpus)
#bigram_tokenizer <- function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "))
#BigramTokenizer <- function(x) {RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 2, max = 2))}
#dtm<-DocumentTermMatrix(corpus, control=list(tokenizer=BigramTokenizer))
}
corpus_train <- create_corpus(data_train)
corpus_test <- create_corpus(data_test)
dtm_train<-DocumentTermMatrix(corpus_train)
tfidf_train <- weightTfIdf(dtm_train)
tfidf_test <- DocumentTermMatrix(corpus_test, control = list(dictionary = Terms(tfidf_train)))
lyrics_test <- as.matrix(tfidf_test)
lyrics_train <- as.matrix(tfidf_train)
library(tidyverse)
library(corrplot)
library(glmnet)
library(Matrix)
library(tidytext)
library(xgboost)
library(data.table)
library(Matrix)
library(patchwork)
library(caret)
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library(glasso)
library(igraph)
library(tidytext)
library(tidyverse)
library(tm)
library(textstem)
library(caret)
library(xgboost)
data$X<-NULL
data$Unnamed..0<-NULL
data$Unnamed..0_x<-NULL
data$Unnamed..0_y<-NULL
data<-data[!data$genre_label=='',]
data<-data[!duplicated(data$track_uri),]
colnames(data)[13]<-"tempo_numeric"
data <- data[data$lyrics_IO==1,]
data$lyrics_IO <- NULL
data$genre_label<-as.factor(data$genre_label)
data$mode<-as.factor(data$mode)
data$key<-as.factor(data$key)
set.seed(1234)
train_dim<-round(NROW(data)*0.8)
random<-sample(1:NROW(data), train_dim)
data_train<-data[random,]
data_test<-data[-random,]
y<-as.factor(data$genre_label)
y.train <- y[random]
y.test <- y[-random]
library(tidyverse)
library(corrplot)
library(glmnet)
library(Matrix)
library(tidytext)
library(xgboost)
library(data.table)
library(Matrix)
library(patchwork)
library(caret)
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library(glasso)
library(igraph)
library(tidytext)
library(tidyverse)
library(tm)
library(textstem)
library(caret)
library(xgboost)
data<-read.csv(file.choose())
data$X<-NULL
data$Unnamed..0<-NULL
data$Unnamed..0_x<-NULL
data$Unnamed..0_y<-NULL
data<-data[!data$genre_label=='',]
data<-data[!duplicated(data$track_uri),]
colnames(data)[13]<-"tempo_numeric"
data <- data[data$lyrics_IO==1,]
data$lyrics_IO <- NULL
data$genre_label<-as.factor(data$genre_label)
data$mode<-as.factor(data$mode)
data$key<-as.factor(data$key)
set.seed(1234)
train_dim<-round(NROW(data)*0.8)
random<-sample(1:NROW(data), train_dim)
data_train<-data[random,]
data_test<-data[-random,]
y<-as.factor(data$genre_label)
y.train <- y[random]
y.test <- y[-random]
# in order to clean up the text all these artifacts
create_corpus <- function(df){
word_dataset_1<-tibble(text=df$lyric)
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\w*Lyrics\\w*", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\w*Contributors\\w*\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\w*contributors\\w*\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\Contributors\\w*\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\contributors\\w*\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="Lyrics", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="lyrics", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\w*Türkçe\\w*\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\w*bokmål\\w*\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\w*Embed\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="[[:punct:]]", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="Chorus", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="chorus", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="Verse", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="verse", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\[.*?\\]", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\(.*?\\)", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\w*\n\\w*", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="[\r\n]", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\w*Contributor\\w*\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\w*contributor\\w*\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\Contributor\\w*\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\contributor\\w*\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="[[:digit:]]+", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\s+", replacement=" ", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="[[:digit:]]", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\bah\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\boh\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\buh\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\bhuh\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\bna\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\byeah\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\bem\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\bmm\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\bwhoa\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="also", replacement="", ignore.case=T))
word_dataset_1 <- word_dataset_1 %>% mutate(text = str_replace_all(text, pattern = "[^A-Za-z\\s]", replacement = ""))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="Travi", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="Scott", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\boh\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\booh\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="\\boohooh\\b", replacement="", ignore.case=T))
word_dataset_1<-word_dataset_1 %>% mutate(text=sapply(text, FUN=gsub, pattern="lllove", replacement="love", ignore.case=T))
# Remove Japanese, Korean, and special characters
x<-VectorSource(word_dataset_1$text)
corpus<-tm::VCorpus(x, readerControl = list(reader = reader(x), language = c('en')))
# Remove Japanese, Korean, and special characters
removeSpecialCharacters <- function(x) {
x <- gsub("[\u3040-\u309F]|[\u30A0-\u30FF]|[\uFF00-\uFF9F]|[\u4E00-\u9FAF]", "", x) # Remove Japanese characters
x <- gsub("[\uAC00-\uD7A3]", "", x) # Remove Korean characters
gsub("[^[:alnum:][:space:]]*", "", x) # Remove special characters
}
corpus <- tm_map(corpus, content_transformer(removeSpecialCharacters))
corpus <- tm_map(corpus, content_transformer(removeSpecialCharacters))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords::stopwords("en", source = "stopwords-iso"))
corpus <- tm_map(corpus, stemDocument, language = c('english'))
corpus <- tm_map(corpus, content_transformer(lemmatize_words), language='en')
return(corpus)
}
corpus_train <- create_corpus(data_train)
corpus_test <- create_corpus(data_test)
dtm_train<-DocumentTermMatrix(corpus_train)
tfidf_train <- weightTfIdf(dtm_train)
tfidf_test <- DocumentTermMatrix(corpus_test, control = list(dictionary = Terms(tfidf_train)))
gc()
data$genre_label<-as.factor(data$genre_label)
data$mode<-as.factor(data$mode)
data$key<-as.factor(data$key)
lyrics_train<-Matrix(as.matrix(tfidf_train), sparse = TRUE)
lyrics_test<-Matrix(as.matrix(tfidf_test), sparse = TRUE)
y<-as.factor(data$genre_label)
word_freq <- col_sums(as.matrix(dtm))
foldid <- sample(1:10, size = length(y[random]), replace = TRUE)
cv1  <- cv.glmnet(train_sparse_matrix, y[random], foldid = foldid, alpha = 1 ,family='multinomial')
data_train_model_matrix<-model.matrix(~.-1, data=data_train[,-c(1,2,3,16,17,18,19)])
data_test_model_matrix<-model.matrix(~.-1, data=data_test[,-c(1,2,3,16,17,18,19)])
train_sparse_matrix<-cbind(data_train_model_matrix, lyrics_train)
test_sparse_matrix<-cbind(data_test_model_matrix, lyrics_test)
foldid <- sample(1:10, size = length(y[random]), replace = TRUE)
cv1  <- cv.glmnet(train_sparse_matrix, y[random], foldid = foldid, alpha = 1 ,family='multinomial')
cv1
confusion.glmnet(cv1, newx = test_sparse_matrix, newy = y[-random])
fit_log_1<-glmnet(x=train_sparse_matrix, y=y[random], family = 'multinomial')
par(mfrow=c(1,3))
plot(fit_log_1)
cv.fit<-cv.glmnet(train_sparse_matrix, y[random], nfold=10, family='multinomial')
par(mfrow=c(1,1))
plot(cv.fit)
par(mfrow=c(1,3))
#abline(v=log(cv.fit$lambda.min), col='red')
plot(fit_log_1, xvar='lambda')
optimal_lambda <- cv.fit$lambda.1se
coef.est1 <- as.matrix(glmnet::coef.glmnet(cv.fit, s=cv.fit$lambda.1se))
class_names <- attr(coef.est1, "dimnames")[[1]]
par(mfrow=c(1,3))
plot(fit_log_1, xvar='lambda')
fit_log_1_ridge<-glmnet(x=train_sparse_matrix, y=y[random], family = 'multinomial', alpha = 0)
par(mfrow=c(1,3))
plot(fit_log_1_ridge)
fit_log_1_el<-glmnet(x=train_sparse_matrix, y=y[random], family = 'multinomial', alpha = 0.75)
par(mfrow=c(1,3))
plot(fit_log_1_el)
# Itera su ogni classe (ogni elemento della lista)
for(i in 1:length(coef.est1)) {
cat("Coefficients for class", class_names[i], ":\n")
# Coefficienti per la classe corrente
coefs_current_class <- coef.est1[[i]]
# Identifica gli indici dei coefficienti non nulli
non_zero_indices <- which(coefs_current_class@x != 0)
# Estrai i nomi delle variabili e i coefficienti corrispondenti
non_zero_coefs <- data.frame(
Variable = rownames(coefs_current_class)[coefs_current_class@i[non_zero_indices] + 1],
Coefficient = coefs_current_class@x[non_zero_indices])
if(i==1){a<-tibble(var=rownames(coefs_current_class)[coefs_current_class@i[non_zero_indices] + 1],
genre=  class_names[i])}
else{a<-a %>% add_row(var=rownames(coefs_current_class)[coefs_current_class@i[non_zero_indices] + 1],
genre=  class_names[i])}
# Stampa i risultati
print(non_zero_coefs)
cat("\n")
}
# here there is some plot for comparison between different coefficients
coef_lasso_min<-coef(cv1, s='lambda.min')
coef_lasso<-coef(cv1, s='lambda.1se')
colnames(train_sparse_matrix)[coef_lasso$pop@i]
selected_word_dataframe_lasso_pop<-tibble(words=colnames(train_sparse_matrix)[coef_lasso$pop@i][-c(1,2,3,4)])
selected_word_dataframe_lasso_pop<-selected_word_dataframe_lasso_pop %>% add_column(value=coef_lasso$pop@x[-c(1,2,3,4,5)])
ggplot(selected_word_dataframe_lasso_pop, aes(x = reorder(words, value), y = value)) +
geom_bar(stat = "identity", fill = "#1DB954") +coord_flip()+
labs(title = "Coefficients of words pop", y = "value", x = "word")
colnames(train_sparse_matrix)[coef_lasso$rap@i]
selected_word_dataframe_lasso_rap<-tibble(words=colnames(train_sparse_matrix)[coef_lasso$rap@i][-c(1,2,3)])
selected_word_dataframe_lasso_rap<-selected_word_dataframe_lasso_rap %>% add_column(value=coef_lasso$rap@x[-c(1,2,3,4)])
ggplot(selected_word_dataframe_lasso_rap, aes(x = reorder(words, value), y = value)) +
geom_bar(stat = "identity", fill = "#1DB954") +coord_flip()+
labs(title = "Coefficients of words rap", y = "value", x = "word")
colnames(train_sparse_matrix)[coef_lasso$rock@i]
selected_word_dataframe_lasso_rock<-tibble(words=colnames(train_sparse_matrix)[coef_lasso$rock@i][-c(1,2,3,4,5,6,7,8,9)])
selected_word_dataframe_lasso_rock<-selected_word_dataframe_lasso_rock %>% add_column(value=coef_lasso$rock@x[-c(1,2,3,4,5,6,7,8,9,10)])
ggplot(selected_word_dataframe_lasso_rock, aes(x = reorder(words, value), y = value)) +
geom_bar(stat = "identity", fill = "#1DB954") +coord_flip()+
labs(title = "Coefficients of words rock", y = "value", x = "word")
numeric_coeff_lasso_pop<-tibble(coef=colnames(train_sparse_matrix)[coef_lasso$pop@i][c(1,2,3,4)],
value=coef_lasso$pop@x[c(2,3,4,5)])
numeric_coeff_lasso_rap<-tibble(coef=colnames(train_sparse_matrix)[coef_lasso$rap@i][c(1,2,3)],
value=coef_lasso$rap@x[c(2,3,4)])
numeric_coeff_lasso_rock<-tibble(coef=colnames(train_sparse_matrix)[coef_lasso$rock@i][c(1,2,3,4,5,6,7,8,9)],
value=coef_lasso$rock@x[c(2,3,4,5,6,7,8,9,10)])
coef_lasso_min<-coef(cv1, s='lambda.min')
colnames(train_sparse_matrix)[coef_lasso_min$pop@i]
selected_word_dataframe_lasso_min_pop<-tibble(words=colnames(train_sparse_matrix)[coef_lasso_min$pop@i][-c(1,2,3,4)])
selected_word_dataframe_lasso_min_pop<-selected_word_dataframe_lasso_min_pop %>% add_column(value=coef_lasso_min$pop@x[-c(1,2,3,4,5)])
ggplot(selected_word_dataframe_lasso_min_pop, aes(x = reorder(words, value), y = value)) +
geom_bar(stat = "identity", fill = "#1DB954") +coord_flip()+
labs(title = "Coefficients of words pop", y = "value", x = "word")
colnames(train_sparse_matrix)[coef_lasso_min$rap@i]
selected_word_dataframe_lasso_min_rap<-tibble(words=colnames(train_sparse_matrix)[coef_lasso_min$rap@i][-c(1,2,3)])
selected_word_dataframe_lasso_min_rap<-selected_word_dataframe_lasso_min_rap %>% add_column(value=coef_lasso_min$rap@x[-c(1,2,3,4)])
ggplot(selected_word_dataframe_lasso_min_rap, aes(x = reorder(words, value), y = value)) +
geom_bar(stat = "identity", fill = "#1DB954") +coord_flip()+
labs(title = "Coefficients of words rap", y = "value", x = "word")
colnames(train_sparse_matrix)[coef_lasso_min$rock@i]
selected_word_dataframe_lasso_min_rock<-tibble(words=colnames(train_sparse_matrix)[coef_lasso_min$rock@i][-c(1,2,3,4,5,6,7,8,9)])
selected_word_dataframe_lasso_min_rock<-selected_word_dataframe_lasso_min_rock %>% add_column(value=coef_lasso_min$rock@x[-c(1,2,3,4,5,6,7,8,9,10)])
ggplot(selected_word_dataframe_lasso_min_rock, aes(x = reorder(words, value), y = value)) +
geom_bar(stat = "identity", fill = "#1DB954") +coord_flip()+
labs(title = "Coefficients of words rock", y = "value", x = "word")
coef.est1 <- as.matrix(glmnet::coef.glmnet(cv1, s='lambda.1se'))
class_names <- attr(coef.est1, "dimnames")[[1]]
confusion.glmnet(cv1, newx = test_sparse_matrix, newy = y[-random])
confusionMatrix(as.factor(predict(cv1, newx = test_sparse_matrix, s='lambda.min', type='class')),
as.factor(y[-random]),
mode = "everything")
confusionMatrix(as.factor(predict(cv1, newx = test_sparse_matrix, s='lambda.min', type='deviance')),
as.factor(y[-random]),
mode = "everything")
?cv.glmnet
cv1  <- cv.glmnet(train_sparse_matrix, y[random], foldid = foldid, alpha = 1 ,family='multinomial',type.measure="deviance")
confusionMatrix(as.factor(predict(cv1, newx = test_sparse_matrix, s='lambda.min', type='class')),
as.factor(y[-random]),
mode = "everything")
confusionMatrix(as.factor(predict(cv1, newx = test_sparse_matrix, s='lambda.1se', type='class')),
as.factor(y[-random]),
mode = "everything")
confusionMatrix(as.factor(predict(cv1, newx = test_sparse_matrix, s='lambda.1se', type='class')),
as.factor(y[-random]),
mode = "everything")
library(forecast)
library(ggplot2)
library(DIMORA)
library(readxl)
gamepassdata <- read.csv("~/Library/Mobile Documents/com~apple~CloudDocs/UNIPD MATERIALI DIDATTICI/DATA SCIENCE/Business economics/project/console/gamepassdata.csv", stringsAsFactors=TRUE)
gp.ts<- ts(gamepassdata$Subscribers.as.of.Month, frequency=12, start=c(2017,7),end=c(2019,12))
# FIRST EXPLORATORY PLOTS
autoplot(gp.ts)+
ggtitle("Gamepass subscribers") +
xlab("") +
ylab("Number of subscriptions")
tsdisplay(gp.ts)
## Simple Bass Model
bm_gp<-BM(gp.ts,display = T)
summary(bm_gp) #R2 very high because the cumulative is almost perfect fitted
setwd("~/Library/Mobile Documents/com~apple~CloudDocs/UNIPD MATERIALI DIDATTICI/DATA SCIENCE/Business economics/project/console/codice")
library(forecast)
library(ggplot2)
library(DIMORA)
library(readxl)
gamepassdata <- read.csv("gamepassdata.csv", stringsAsFactors=TRUE)
gp.ts<- ts(gamepassdata$Subscribers.as.of.Month, frequency=12, start=c(2017,7),end=c(2019,12))
# FIRST EXPLORATORY PLOTS
autoplot(gp.ts)+
ggtitle("Gamepass subscribers") +
xlab("") +
ylab("Number of subscriptions")
fc<- holt(gp.ts, h=15)
fc2<- holt(gp.ts, damped=T, phi=0.9, h=15)
summary(fc)
autoplot(gp.ts)+
autolayer(fc, series="Holt's method", PI=F)+
autolayer(fc2, series="Damped Holt's method", PI=F)+ ylab("") + xlab("")
fit1<- hw(gp.ts, seasonal="additive")
fit2<- hw(gp.ts, seasonal="multiplicative")
autoplot(gp.ts)+
autolayer(fit2, series="", PI=F)+
#autolayer(fit2, series="Holt-Winters with multiplicative seasonality", PI=F)+
ylab("") + xlab("")
summary(fit2)
summary(fit1)
auto<- auto.arima(gp.ts)
summary(auto)
summary(fc)
summary(fc2)
